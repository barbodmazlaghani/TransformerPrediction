{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "z45c8di4qOt0",
        "outputId": "fc825eb9-a452-47f0-c3de-01d132ccf53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/DataAugumentation/original'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dbd9500222c8>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'augmented'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/DataAugumentation/original'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp \"/content/gdrive/My Drive/DataAugumentation.zip\" .\n",
        "!unzip -qq DataAugumentation.zip\n",
        "!rm DataAugumentation.zip\n",
        "data_path = 'DataAugumentation'\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Embedding, GlobalAveragePooling1D\n",
        "\n",
        "SEQUENCE_LENGTH = 600\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "    features = df[['Engine speed', 'Throttle position', 'Accelerator pedal position', 'Speed']]\n",
        "    target = df['Momentary fuel consumption']\n",
        "    features = features.iloc[:SEQUENCE_LENGTH]\n",
        "    target = target.iloc[:SEQUENCE_LENGTH]\n",
        "    return features.values, target.values\n",
        "\n",
        "def pad_and_normalize(data, scaler, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = pad_sequences(data, maxlen=sequence_length, dtype='float32', padding='post', truncating='post')\n",
        "    normalized_data = scaler.transform(padded_data.reshape(-1, padded_data.shape[-1])).reshape(padded_data.shape)\n",
        "    return normalized_data\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_original = []\n",
        "y_original = []\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "\n",
        "SEQUENCE_LENGTH = 600\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "PLOT_SAVE_DIR = 'predicted_vs_actual_plots'\n",
        "\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df['Time'] = df['Time'] - df['Time'].iloc[0]\n",
        "\n",
        "    df['Momentary fuel consumption'] = df['Trip fuel consumption'].diff().fillna(0)\n",
        "\n",
        "    features = df[['Engine speed', 'Throttle position', 'Accelerator pedal position', 'Speed']]\n",
        "    target = df['Momentary fuel consumption']\n",
        "\n",
        "    features = features.iloc[:SEQUENCE_LENGTH]\n",
        "    target = target.iloc[:SEQUENCE_LENGTH]\n",
        "\n",
        "    return features.values, target.values\n",
        "\n",
        "\n",
        "\n",
        "def pad_and_normalize(data, scaler, sequence_length=SEQUENCE_LENGTH):\n",
        "    padded_data = pad_sequences(data, maxlen=sequence_length, dtype='float32', padding='post', truncating='post')\n",
        "    normalized_data = scaler.transform(padded_data.reshape(-1, padded_data.shape[-1])).reshape(padded_data.shape)\n",
        "    return normalized_data\n",
        "\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_original = []\n",
        "y_original = []\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n",
        "base_folder_path = '/content/'\n",
        "for i in range(5):\n",
        "    folder_path = os.path.join(base_folder_path, f'data_aug(3_slices_with_repeated)_cluster_{i}')\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            features, target = process_file(file_path)\n",
        "\n",
        "            slices = filename.split('_')\n",
        "            is_original_trip = slices[0] == slices[2] and slices[0] == slices[4]\n",
        "\n",
        "            if is_original_trip:\n",
        "                X_original.append(features)\n",
        "                y_original.append(target)\n",
        "            else:\n",
        "                X_augmented.append(features)\n",
        "                y_augmented.append(target)\n",
        "\n",
        "# Pad and convert lists to numpy arrays\n",
        "X_original = pad_sequences(X_original, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "y_original = pad_sequences(y_original, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "X_augmented = pad_sequences(X_augmented, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "y_augmented = pad_sequences(y_augmented, maxlen=SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post')\n",
        "\n",
        "num_test = int(0.2 * len(X_original))\n",
        "X_test = X_original[:num_test]\n",
        "y_test = y_original[:num_test]\n",
        "X_train = np.concatenate([X_original[num_test:], X_augmented])\n",
        "y_train = np.concatenate([y_original[num_test:], y_augmented])\n",
        "\n",
        "scaler_X.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
        "scaler_y.fit(y_train.reshape(-1, 1))\n",
        "\n",
        "joblib.dump(scaler_X, 'scaler_X.pkl')\n",
        "joblib.dump(scaler_y, 'scaler_y.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOAiXus9rXi7",
        "outputId": "4a54927c-a3e8-4b3d-f109-50689a572052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler_y.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_normalized = pad_and_normalize(X_train, scaler_X)\n",
        "y_train_normalized = scaler_y.transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
        "X_test_normalized = pad_and_normalize(X_test, scaler_X)\n",
        "y_test_normalized = scaler_y.transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
        "\n",
        "print('X_train_normalized shape:', X_train_normalized.shape)\n",
        "print('y_train_normalized shape:', y_train_normalized.shape)\n",
        "print('X_test_normalized shape:', X_test_normalized.shape)\n",
        "print('y_test_normalized shape:', y_test_normalized.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aW5BJRBsInm",
        "outputId": "449f086e-d4cd-4013-9096-65274e2fb97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_normalized shape: (28713, 600, 4)\n",
            "y_train_normalized shape: (28713, 600)\n",
            "X_test_normalized shape: (15, 600, 4)\n",
            "y_test_normalized shape: (15, 600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout, MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 600\n",
        "FEATURE_DIM = 4\n",
        "NUM_HEADS = 8\n",
        "FF_DIM = 32\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "        self.pos_encoding = self.positional_encoding(sequence_length, output_dim)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
        "                                     np.arange(d_model)[np.newaxis, :],\n",
        "                                     d_model)\n",
        "        sines = np.sin(angle_rads[:, 0::2])\n",
        "        cosines = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "# Custom Transformer Block Layer\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, num_heads, ff_dim, rate=0.2):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)\n",
        "        self.ffn = Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(FEATURE_DIM)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Building the model\n",
        "model = Sequential([\n",
        "    tf.keras.layers.InputLayer(shape=(SEQUENCE_LENGTH, FEATURE_DIM)),\n",
        "    PositionalEncoding(SEQUENCE_LENGTH, FEATURE_DIM),\n",
        "    TransformerBlock(NUM_HEADS, FF_DIM),\n",
        "    TransformerBlock(NUM_HEADS, FF_DIM),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error')\n",
        "\n",
        "# Show the model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(X_train_normalized, y_train_normalized, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "vE-iTm6wqooQ",
        "outputId": "44edbe7e-c5ab-4966-cfe0-b1058c2d882f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ positional_encoding_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m5,176\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_1                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m5,176\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │               \u001b[38;5;34m5\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ positional_encoding_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,176</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_1                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,176</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,357\u001b[0m (40.46 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,357</span> (40.46 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,357\u001b[0m (40.46 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,357</span> (40.46 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 98ms/step - loss: 0.1333 - val_loss: 0.0104\n",
            "Epoch 2/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 81ms/step - loss: 0.0112 - val_loss: 0.0104\n",
            "Epoch 3/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.0110 - val_loss: 0.0103\n",
            "Epoch 4/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.0104 - val_loss: 0.0077\n",
            "Epoch 5/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 78ms/step - loss: 0.0086 - val_loss: 0.0069\n",
            "Epoch 6/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 82ms/step - loss: 0.0080 - val_loss: 0.0069\n",
            "Epoch 7/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.0078 - val_loss: 0.0068\n",
            "Epoch 8/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.0077 - val_loss: 0.0069\n",
            "Epoch 9/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.0076 - val_loss: 0.0069\n",
            "Epoch 10/10\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 78ms/step - loss: 0.0075 - val_loss: 0.0068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c19553d8c70>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('modelTransformer.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMJgqNx25A6G",
        "outputId": "09ed79af-1fab-4a08-c9c8-d5659e8991e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = model.evaluate(X_test_normalized, y_test_normalized)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "y_pred_normalized = model.predict(X_test_normalized)\n",
        "\n",
        "y_test_inv = scaler_y.inverse_transform(y_test_normalized.reshape(-1, SEQUENCE_LENGTH))\n",
        "y_pred_inv = scaler_y.inverse_transform(y_pred_normalized.reshape(-1, SEQUENCE_LENGTH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lWVaYjO43D8",
        "outputId": "de28dc3a-c5f4-47a4-dd9f-50136b512791"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0026\n",
            "Test Loss: 0.002647922607138753\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(PLOT_SAVE_DIR):\n",
        "    os.makedirs(PLOT_SAVE_DIR)\n",
        "\n",
        "for i in range(len(X_test_normalized)):\n",
        "    # print(y_test_inv[i])\n",
        "    # print(y_pred_inv[i])\n",
        "    # input()\n",
        "    cumulative_actuals = np.cumsum(y_test_inv[i], axis=0)\n",
        "    cumulative_predictions = np.cumsum(y_pred_inv[i], axis=0)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(cumulative_actuals, label='Actual Trip Fuel Consumption')\n",
        "    plt.plot(cumulative_predictions, label='Predicted Trip Fuel Consumption')\n",
        "    plt.title(f'Trip {i + 1}: Actual vs Predicted Fuel Consumption')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Trip Fuel Consumption (uL)')\n",
        "    plt.legend()\n",
        "    plot_filename = os.path.join(PLOT_SAVE_DIR, f'trip_{i + 1}_actual_vs_predicted.png')\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()\n",
        "\n",
        "!zip -r data.zip predicted_vs_actual_plots"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92JRsyhcqsI6",
        "outputId": "15652a3b-61f7-45b3-f528-a4648daa4329"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: predicted_vs_actual_plots/ (stored 0%)\n",
            "updating: predicted_vs_actual_plots/trip_7_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_3_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_6_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_14_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_10_actual_vs_predicted.png (deflated 11%)\n",
            "updating: predicted_vs_actual_plots/trip_15_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_5_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_12_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_1_actual_vs_predicted.png (deflated 10%)\n",
            "updating: predicted_vs_actual_plots/trip_11_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_4_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_8_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_13_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_9_actual_vs_predicted.png (deflated 9%)\n",
            "updating: predicted_vs_actual_plots/trip_2_actual_vs_predicted.png (deflated 9%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7ly2Iio--PS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}